{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "XZ34cfk213Ux"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(vocab_size=10000):\n",
        "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "    print(f\"Training samples: {len(X_train)}, Testing samples: {len(X_test)}\")\n",
        "    return (X_train, y_train), (X_test, y_test)\n",
        "\n",
        "def word_index_mapping():\n",
        "    word_index = imdb.get_word_index()\n",
        "    reverse_word_index = {value + 3: key for key, value in word_index.items()}\n",
        "    reverse_word_index[0] = '<PAD>'\n",
        "    reverse_word_index[1] = '<START>'\n",
        "    reverse_word_index[2] = '<UNK>'\n",
        "    return word_index, reverse_word_index\n",
        "\n",
        "def decode_review(encoded_review, reverse_word_index):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in encoded_review])\n",
        "\n",
        "def get_decoded_training_data(X_train, reverse_word_index):\n",
        "    decoded_reviews = []\n",
        "    for review in X_train:\n",
        "        decoded_reviews.append(decode_review(review, reverse_word_index))\n",
        "    return decoded_reviews\n",
        "\n",
        "def preprocess_data(X_train, X_test, max_length=250):\n",
        "    X_train_padded = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
        "    X_test_padded = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
        "    return X_train_padded, X_test_padded"
      ],
      "metadata": {
        "id": "6a9ylnI-1-Fh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, max_length):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dense(16, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_model(model, X_train_padded, y_train, epochs=5, batch_size=128):\n",
        "    history = model.fit(\n",
        "        X_train_padded, y_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.2,\n",
        "        verbose=1\n",
        "    )\n",
        "    return history\n",
        "\n",
        "def evaluate_model(model, X_test_padded, y_test):\n",
        "    loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
        "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
        "    return loss, accuracy\n",
        "\n",
        "def create_tokenizer(decoded_reviews, vocab_size):\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "    tokenizer.fit_on_texts(decoded_reviews)\n",
        "    return tokenizer\n"
      ],
      "metadata": {
        "id": "EFwk87aO2Lna"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(model, text=None, X_test=None, X_test_padded=None, y_test=None,\n",
        "                      max_length=250, reverse_word_index=None, tokenizer=None):\n",
        "    if text:\n",
        "        sequence = tokenizer.texts_to_sequences([text])\n",
        "        padded = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
        "    else:\n",
        "        review_index = np.random.randint(0, len(X_test))\n",
        "        padded = X_test_padded[review_index:review_index+1]\n",
        "        text = decode_review(X_test[review_index], reverse_word_index)\n",
        "        actual_sentiment = 'Positive' if y_test[review_index] == 1 else 'Negative'\n",
        "        print(f\"Actual sentiment: {actual_sentiment}\")\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict(padded, verbose=0)[0][0]\n",
        "    sentiment = \"Positive\" if prediction >= 0.5 else \"Negative\"\n",
        "    confidence = prediction if prediction >= 0.5 else 1 - prediction\n",
        "\n",
        "    return {\n",
        "        \"text\": text,\n",
        "        \"sentiment\": sentiment,\n",
        "        \"confidence\": float(confidence)\n",
        "    }"
      ],
      "metadata": {
        "id": "2UOjCSkX2d9L"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_examples(model, X_test, X_test_padded, y_test, reverse_word_index, tokenizer, max_length):\n",
        "\n",
        "    custom_examples = [\n",
        "        \"This was the best purchase I ever made\",\n",
        "        \"This was the worst purchase I ever made.\"\n",
        "    ]\n",
        "\n",
        "    for example in custom_examples:\n",
        "        result = predict_sentiment(\n",
        "            model, text=example, tokenizer=tokenizer, max_length=max_length\n",
        "        )\n",
        "        print(f\"Text: '{result['text']}'\")\n",
        "        print(f\"Predicted sentiment: {result['sentiment']} (confidence: {result['confidence']:.4f})\")\n",
        "        print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "FIPKEwHY2kiF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    vocab_size = 10000\n",
        "    embedding_dim = 16\n",
        "    max_length = 250\n",
        "    epochs = 5\n",
        "    batch_size = 128\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = load_data(vocab_size)\n",
        "    word_index, reverse_word_index = create_word_index_mapping()\n",
        "\n",
        "    decoded_reviews = get_decoded_training_data(X_train, reverse_word_index)\n",
        "\n",
        "    tokenizer = create_tokenizer(decoded_reviews, vocab_size)\n",
        "\n",
        "    X_train_padded, X_test_padded = preprocess_data(X_train, X_test, max_length)\n",
        "\n",
        "    model = build_model(vocab_size, embedding_dim, max_length)\n",
        "    train_model(model, X_train_padded, y_train, epochs, batch_size)\n",
        "    evaluate_model(model, X_test_padded, y_test)\n",
        "\n",
        "    # Test the model\n",
        "    test_examples(model, X_test, X_test_padded, y_test, reverse_word_index, tokenizer, max_length)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0otho_f2o9N",
        "outputId": "a42eca4d-8e52-4282-b747-1cd422c83e34"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 25000, Testing samples: 25000\n",
            "Epoch 1/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.6112 - loss: 0.6841 - val_accuracy: 0.7424 - val_loss: 0.6098\n",
            "Epoch 2/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7896 - loss: 0.5616 - val_accuracy: 0.8390 - val_loss: 0.4337\n",
            "Epoch 3/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.8501 - loss: 0.3993 - val_accuracy: 0.8626 - val_loss: 0.3534\n",
            "Epoch 4/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8775 - loss: 0.3176 - val_accuracy: 0.8752 - val_loss: 0.3174\n",
            "Epoch 5/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.8973 - loss: 0.2727 - val_accuracy: 0.8502 - val_loss: 0.3310\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8464 - loss: 0.3369\n",
            "Test accuracy: 0.8454\n",
            "Text: 'This was the best purchase I ever made'\n",
            "Predicted sentiment: Negative (confidence: 0.7174)\n",
            "----------------------------------------\n",
            "Text: 'This was the worst purchase I ever made.'\n",
            "Predicted sentiment: Negative (confidence: 0.8989)\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ]
}